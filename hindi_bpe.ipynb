{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRQEVMlhk-0d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Set data directory\n",
        "DATA_DIR = os.path.join('data', 'hindi')\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean the text by removing unnecessary whitespace and special characters.\"\"\"\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    # Remove special characters except Hindi Unicode range and basic punctuation\n",
        "    text = re.sub(r'[^\\u0900-\\u097F\\s\\.,\\?!]', ' ', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def download_hindi_text():\n",
        "    \"\"\"Download Hindi text from Hindi Wikipedia featured articles.\"\"\"\n",
        "    # URLs of some Hindi Wikipedia featured articles\n",
        "    urls = [\n",
        "        'https://hi.wikipedia.org/wiki/भारत',\n",
        "        'https://hi.wikipedia.org/wiki/हिन्दी',\n",
        "        'https://hi.wikipedia.org/wiki/दिल्ली',\n",
        "        'https://hi.wikipedia.org/wiki/महात्मा_गांधी',\n",
        "        'https://hi.wikipedia.org/wiki/योग',\n",
        "        'https://hi.wikipedia.org/wiki/भारतीय_संविधान',\n",
        "        'https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति',\n",
        "        'https://hi.wikipedia.org/wiki/भारतीय_संविधान_सभा',\n",
        "        'https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव',\n",
        "        'https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_2017',\n",
        "        'https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_2012',\n",
        "        'https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_2007',\n",
        "        'https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_2002',\n",
        "        'https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_1997',\n",
        "    ]\n",
        "\n",
        "    all_text = []\n",
        "\n",
        "    for url in urls:\n",
        "        try:\n",
        "            print(f\"Downloading from {url}\")\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Get main content\n",
        "            content = soup.find(id='mw-content-text')\n",
        "            if content:\n",
        "                paragraphs = content.find_all('p')\n",
        "                text = ' '.join(p.get_text() for p in paragraphs)\n",
        "                cleaned_text = clean_text(text)\n",
        "                if cleaned_text:\n",
        "                    all_text.append(cleaned_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {url}: {e}\")\n",
        "\n",
        "    # Combine all text\n",
        "    combined_text = ' '.join(all_text)\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "    # Save to file\n",
        "    with open(os.path.join(DATA_DIR, 'text.txt'), 'w', encoding='utf-8') as f:\n",
        "        f.write(combined_text)\n",
        "\n",
        "    print(f\"\\nDownloaded and saved {len(combined_text)} characters of Hindi text\")\n",
        "    return combined_text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    download_hindi_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtz49UIClIrR",
        "outputId": "19b9f2f2-03dd-4a54-adfd-d8e691e8b2f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://hi.wikipedia.org/wiki/भारत\n",
            "Downloading from https://hi.wikipedia.org/wiki/हिन्दी\n",
            "Downloading from https://hi.wikipedia.org/wiki/दिल्ली\n",
            "Downloading from https://hi.wikipedia.org/wiki/महात्मा_गांधी\n",
            "Downloading from https://hi.wikipedia.org/wiki/योग\n",
            "Downloading from https://hi.wikipedia.org/wiki/भारतीय_संविधान\n",
            "Downloading from https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति\n",
            "Downloading from https://hi.wikipedia.org/wiki/भारतीय_संविधान_सभा\n",
            "Downloading from https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव\n",
            "Error downloading https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव: 404 Client Error: Not Found for url: https://hi.wikipedia.org/wiki/%E0%A4%AD%E0%A4%BE%E0%A4%B0%E0%A4%A4%E0%A5%80%E0%A4%AF_%E0%A4%B0%E0%A4%BE%E0%A4%B7%E0%A5%8D%E0%A4%9F%E0%A5%8D%E0%A4%B0%E0%A4%AA%E0%A4%A4%E0%A4%BF_%E0%A4%9A%E0%A5%81%E0%A4%A8%E0%A4%BE%E0%A4%B5\n",
            "Downloading from https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_2017\n",
            "Error downloading https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_2017: 404 Client Error: Not Found for url: https://hi.wikipedia.org/wiki/%E0%A4%AD%E0%A4%BE%E0%A4%B0%E0%A4%A4%E0%A5%80%E0%A4%AF_%E0%A4%B0%E0%A4%BE%E0%A4%B7%E0%A5%8D%E0%A4%9F%E0%A5%8D%E0%A4%B0%E0%A4%AA%E0%A4%A4%E0%A4%BF_%E0%A4%9A%E0%A5%81%E0%A4%A8%E0%A4%BE%E0%A4%B5_2017\n",
            "Downloading from https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_2012\n",
            "Error downloading https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_2012: 404 Client Error: Not Found for url: https://hi.wikipedia.org/wiki/%E0%A4%AD%E0%A4%BE%E0%A4%B0%E0%A4%A4%E0%A5%80%E0%A4%AF_%E0%A4%B0%E0%A4%BE%E0%A4%B7%E0%A5%8D%E0%A4%9F%E0%A5%8D%E0%A4%B0%E0%A4%AA%E0%A4%A4%E0%A4%BF_%E0%A4%9A%E0%A5%81%E0%A4%A8%E0%A4%BE%E0%A4%B5_2012\n",
            "Downloading from https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_2007\n",
            "Error downloading https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_2007: 404 Client Error: Not Found for url: https://hi.wikipedia.org/wiki/%E0%A4%AD%E0%A4%BE%E0%A4%B0%E0%A4%A4%E0%A5%80%E0%A4%AF_%E0%A4%B0%E0%A4%BE%E0%A4%B7%E0%A5%8D%E0%A4%9F%E0%A5%8D%E0%A4%B0%E0%A4%AA%E0%A4%A4%E0%A4%BF_%E0%A4%9A%E0%A5%81%E0%A4%A8%E0%A4%BE%E0%A4%B5_2007\n",
            "Downloading from https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_2002\n",
            "Error downloading https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_2002: 404 Client Error: Not Found for url: https://hi.wikipedia.org/wiki/%E0%A4%AD%E0%A4%BE%E0%A4%B0%E0%A4%A4%E0%A5%80%E0%A4%AF_%E0%A4%B0%E0%A4%BE%E0%A4%B7%E0%A5%8D%E0%A4%9F%E0%A5%8D%E0%A4%B0%E0%A4%AA%E0%A4%A4%E0%A4%BF_%E0%A4%9A%E0%A5%81%E0%A4%A8%E0%A4%BE%E0%A4%B5_2002\n",
            "Downloading from https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_1997\n",
            "Error downloading https://hi.wikipedia.org/wiki/भारतीय_राष्ट्रपति_चुनाव_1997: 404 Client Error: Not Found for url: https://hi.wikipedia.org/wiki/%E0%A4%AD%E0%A4%BE%E0%A4%B0%E0%A4%A4%E0%A5%80%E0%A4%AF_%E0%A4%B0%E0%A4%BE%E0%A4%B7%E0%A5%8D%E0%A4%9F%E0%A5%8D%E0%A4%B0%E0%A4%AA%E0%A4%A4%E0%A4%BF_%E0%A4%9A%E0%A5%81%E0%A4%A8%E0%A4%BE%E0%A4%B5_1997\n",
            "\n",
            "Downloaded and saved 206308 characters of Hindi text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "class BPEVisualizer:\n",
        "    \"\"\"Visualizes BPE training statistics.\"\"\"\n",
        "\n",
        "    def __init__(self, stats_dir: str):\n",
        "        self.stats_dir = stats_dir\n",
        "        self.plots_dir = os.path.join(stats_dir, 'plots')\n",
        "        os.makedirs(self.plots_dir, exist_ok=True)\n",
        "\n",
        "    def plot_training_stats(self, metrics_logger):\n",
        "        \"\"\"Generate all training statistics plots.\"\"\"\n",
        "        # Get data from metrics\n",
        "        iterations = [log['iteration'] for log in metrics_logger.token_logs]\n",
        "        vocab_sizes = [log['vocab_size'] for log in metrics_logger.token_logs]\n",
        "        token_freqs = [log['frequency'] for log in metrics_logger.token_logs]\n",
        "        compression_ratios = [log['compression_ratio'] for log in metrics_logger.compression_logs]\n",
        "\n",
        "        # Create figure with subplots\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Vocabulary Size Growth\n",
        "        ax1 = fig.add_subplot(221)\n",
        "        ax1.plot(iterations, vocab_sizes)\n",
        "        ax1.set_title('Vocabulary Size Growth')\n",
        "        ax1.set_xlabel('Iteration')\n",
        "        ax1.set_ylabel('Vocabulary Size')\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # 2. Compression Ratio Progress\n",
        "        ax2 = fig.add_subplot(222)\n",
        "        ax2.plot(iterations, compression_ratios)\n",
        "        ax2.set_title('Compression Ratio Progress')\n",
        "        ax2.set_xlabel('Iteration')\n",
        "        ax2.set_ylabel('Compression Ratio')\n",
        "        ax2.grid(True)\n",
        "\n",
        "        # 3. Token Frequencies (Log Scale)\n",
        "        ax3 = fig.add_subplot(223)\n",
        "        ax3.plot(iterations, token_freqs)\n",
        "        ax3.set_title('Token Frequencies')\n",
        "        ax3.set_xlabel('Iteration')\n",
        "        ax3.set_ylabel('Frequency')\n",
        "        ax3.set_yscale('log')\n",
        "        ax3.grid(True)\n",
        "\n",
        "        # 4. Compression Ratio Distribution\n",
        "        ax4 = fig.add_subplot(224)\n",
        "        ax4.hist(compression_ratios, bins=50)\n",
        "        ax4.set_title('Compression Ratio Distribution')\n",
        "        ax4.set_xlabel('Compression Ratio')\n",
        "        ax4.set_ylabel('Count')\n",
        "        ax4.grid(True)\n",
        "\n",
        "        # Save combined plot\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.plots_dir, 'training_stats.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Save individual plots\n",
        "        self._save_individual_plots(\n",
        "            iterations, vocab_sizes, compression_ratios, token_freqs\n",
        "        )\n",
        "\n",
        "    def _save_individual_plots(self, iterations, vocab_sizes, compression_ratios, token_freqs):\n",
        "        \"\"\"Save individual plots for each metric.\"\"\"\n",
        "        # Vocabulary Size\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(iterations, vocab_sizes)\n",
        "        plt.title('Vocabulary Size Growth')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Vocabulary Size')\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.plots_dir, 'vocab_size.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Compression Ratio\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(iterations, compression_ratios)\n",
        "        plt.title('Compression Ratio Progress')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Compression Ratio')\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.plots_dir, 'compression_ratio.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Token Frequencies\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(iterations, token_freqs)\n",
        "        plt.title('Token Frequencies')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.yscale('log')\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.plots_dir, 'token_frequencies.png'))\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "9hbLjQg4mlNm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Set, Dict, Tuple\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "class BaseTokenizer:\n",
        "    \"\"\"Base class for tokenizer implementations.\"\"\"\n",
        "    def __init__(self, vocab_size: int):\n",
        "        if vocab_size <= 0:\n",
        "            raise ValueError(\"Vocabulary size must be positive\")\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab: Set[str] = set()\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Convert text into list of tokens.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def detokenize(self, tokens: List[str]) -> str:\n",
        "        \"\"\"Convert tokens back to text.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "class CharacterTokenizer(BaseTokenizer):\n",
        "    \"\"\"Simple character-level tokenizer.\"\"\"\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        return list(text)\n",
        "\n",
        "    def detokenize(self, tokens: List[str]) -> str:\n",
        "        return ''.join(tokens)"
      ],
      "metadata": {
        "id": "CYK4jhjnmtKH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "import json\n",
        "\n",
        "@dataclass\n",
        "class TrainingMetrics:\n",
        "    \"\"\"Container for training metrics.\"\"\"\n",
        "    iteration: int\n",
        "    vocab_size: int\n",
        "    tokens: int\n",
        "    new_token: str\n",
        "    frequency: int\n",
        "    compression_ratio: float\n",
        "\n",
        "class MetricsLogger:\n",
        "    \"\"\"Handles logging and saving of training metrics.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.token_logs: List[Dict] = []\n",
        "        self.compression_logs: List[Dict] = []\n",
        "\n",
        "    def log_iteration(self, metrics: TrainingMetrics):\n",
        "        \"\"\"Log metrics for current iteration.\"\"\"\n",
        "        self.token_logs.append({\n",
        "            'iteration': metrics.iteration,\n",
        "            'vocab_size': metrics.vocab_size,\n",
        "            'tokens': metrics.tokens,\n",
        "            'new_token': metrics.new_token,\n",
        "            'frequency': metrics.frequency\n",
        "        })\n",
        "\n",
        "        self.compression_logs.append({\n",
        "            'iteration': metrics.iteration,\n",
        "            'compression_ratio': metrics.compression_ratio\n",
        "        })\n",
        "\n",
        "    def print_progress(self, metrics: TrainingMetrics, force: bool = False):\n",
        "        \"\"\"Print training progress.\"\"\"\n",
        "        if force or metrics.iteration % 500 == 0:\n",
        "            print(f\"\\nIteration {metrics.iteration:,}:\")\n",
        "            print(f\"Vocab size: {metrics.vocab_size:,}\")\n",
        "            print(f\"Compression ratio: {metrics.compression_ratio:.2f}\")\n",
        "            print(f\"New token: {metrics.new_token} (freq: {metrics.frequency:,})\")\n",
        "            print(f\"Current tokens: {metrics.tokens:,}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\"Save metrics to file.\"\"\"\n",
        "        data = {\n",
        "            'token_logs': self.token_logs,\n",
        "            'compression_logs': self.compression_logs\n",
        "        }\n",
        "        with open(path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "SqKjBwSXmwhn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Dict, Set\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "class HindiBPE(BaseTokenizer):\n",
        "    \"\"\"Byte-Pair Encoding implementation for Hindi text.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int = 5000):\n",
        "        super().__init__(vocab_size)\n",
        "        self.merges: Dict[Tuple[str, str], str] = {}\n",
        "        self.metrics = MetricsLogger()\n",
        "\n",
        "    def get_stats(self, words: List[List[str]]) -> Counter:\n",
        "        \"\"\"Count pair frequencies in current vocabulary.\"\"\"\n",
        "        pairs = Counter()\n",
        "        for word in words:\n",
        "            for i in range(len(word) - 1):\n",
        "                pairs[tuple(word[i:i + 2])] += 1\n",
        "        return pairs\n",
        "\n",
        "    def merge_vocab(self, words: List[List[str]], pair: Tuple[str, str], new_token: str) -> List[List[str]]:\n",
        "        \"\"\"Merge all occurrences of a pair into a new token.\"\"\"\n",
        "        new_words = []\n",
        "        bigram = re.escape(' '.join(pair))\n",
        "        pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "\n",
        "        for word in words:\n",
        "            w = ' '.join(word)\n",
        "            w = pattern.sub(new_token, w)\n",
        "            new_words.append(w.split())\n",
        "\n",
        "        return new_words\n",
        "\n",
        "    def fit(self, text: str, min_freq: int = 2):\n",
        "        \"\"\"Train BPE on input text.\"\"\"\n",
        "        if not text or not text.strip():\n",
        "            raise ValueError(\"Input text cannot be empty\")\n",
        "\n",
        "        # Initialize with characters\n",
        "        words = [[c for c in word] for word in text.split()]\n",
        "        self.vocab = set(char for word in words for char in word)\n",
        "\n",
        "        original_tokens = sum(len(word) for word in words)\n",
        "\n",
        "        iteration = 0\n",
        "        while len(self.vocab) < self.vocab_size:\n",
        "            pairs = self.get_stats(words)\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            most_common = pairs.most_common(1)[0]\n",
        "            if most_common[1] < min_freq:\n",
        "                break\n",
        "\n",
        "            pair, count = most_common\n",
        "            new_token = ''.join(pair)\n",
        "            self.merges[pair] = new_token\n",
        "            self.vocab.add(new_token)\n",
        "\n",
        "            words = self.merge_vocab(words, pair, new_token)\n",
        "\n",
        "            # Calculate metrics\n",
        "            current_tokens = sum(len(word) for word in words)\n",
        "            metrics = TrainingMetrics(\n",
        "                iteration=iteration,\n",
        "                vocab_size=len(self.vocab),\n",
        "                tokens=current_tokens,\n",
        "                new_token=new_token,\n",
        "                frequency=count,\n",
        "                compression_ratio=original_tokens / current_tokens\n",
        "            )\n",
        "\n",
        "            # Log metrics\n",
        "            self.metrics.log_iteration(metrics)\n",
        "            self.metrics.print_progress(metrics)\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "        # Print final statistics\n",
        "        final_metrics = TrainingMetrics(\n",
        "            iteration=iteration,\n",
        "            vocab_size=len(self.vocab),\n",
        "            tokens=current_tokens,\n",
        "            new_token=new_token,\n",
        "            frequency=count,\n",
        "            compression_ratio=original_tokens / current_tokens\n",
        "        )\n",
        "        self.metrics.print_progress(final_metrics, force=True)\n",
        "\n",
        "    def encode(self, text: str) -> List[str]:\n",
        "        \"\"\"Encode text using learned BPE merges.\"\"\"\n",
        "        words = [[c for c in word] for word in text.split()]\n",
        "        for pair, new_token in self.merges.items():\n",
        "            words = self.merge_vocab(words, pair, new_token)\n",
        "        return [token for word in words for token in word]\n",
        "\n",
        "    def decode(self, tokens: List[str]) -> str:\n",
        "        \"\"\"Decode tokens back to text.\"\"\"\n",
        "        return ' '.join(''.join(tokens))\n",
        "\n",
        "    def save(self, model_path: str, stats_path: str = None):\n",
        "        \"\"\"Save BPE model to file.\"\"\"\n",
        "        data = {\n",
        "            'merges': {' '.join(k): v for k, v in self.merges.items()},\n",
        "            'vocab': list(self.vocab)\n",
        "        }\n",
        "        with open(model_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Save metrics separately\n",
        "        if stats_path:\n",
        "            self.metrics.save(stats_path)\n",
        "\n",
        "    def load(self, model_path: str, stats_path: str = None):\n",
        "        \"\"\"Load BPE model from file.\"\"\"\n",
        "        with open(model_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        self.merges = {tuple(k.split()): v for k, v in data['merges'].items()}\n",
        "        self.vocab = set(data['vocab'])\n",
        "\n",
        "        # Load metrics if available\n",
        "        if stats_path and os.path.exists(stats_path):\n",
        "            with open(stats_path, 'r', encoding='utf-8') as f:\n",
        "                metrics_data = json.load(f)\n",
        "                self.metrics.token_logs = metrics_data['token_logs']\n",
        "                self.metrics.compression_logs = metrics_data['compression_logs']\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "HTCtL4Zqm0Hm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def create_directory_structure():\n",
        "    \"\"\"Create the required directory structure.\"\"\"\n",
        "    directories = [\n",
        "        os.path.join('models', 'hindi_bpe'),\n",
        "        os.path.join('stats', 'hindi_bpe'),\n",
        "        os.path.join('stats', 'hindi_bpe', 'plots'),\n",
        "        os.path.join('data', 'hindi'),\n",
        "    ]\n",
        "\n",
        "    for directory in directories:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "        print(f\"Created directory: {directory}\")\n",
        "\n",
        "# Create directories if they don't exist\n",
        "MODEL_DIR = os.path.join('models', 'hindi_bpe')\n",
        "STATS_DIR = os.path.join('stats', 'hindi_bpe')\n",
        "DATA_DIR = os.path.join('data', 'hindi')\n",
        "create_directory_structure()\n",
        "\n",
        "def main():\n",
        "    # Load Hindi text data\n",
        "    with open(os.path.join(DATA_DIR, 'text.txt'), 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Initialize and train BPE\n",
        "    bpe = HindiBPE(vocab_size=5500)\n",
        "    bpe.fit(text)\n",
        "\n",
        "    # Save the model and metrics\n",
        "    model_path = os.path.join(MODEL_DIR, 'model.json')\n",
        "    stats_path = os.path.join(STATS_DIR, 'metrics.json')\n",
        "    bpe.save(model_path, stats_path)\n",
        "\n",
        "    # Generate and save visualization plots\n",
        "    visualizer = BPEVisualizer(STATS_DIR)\n",
        "    visualizer.plot_training_stats(bpe.metrics)\n",
        "    print(\"\\nGenerated visualization plots in:\", os.path.join(STATS_DIR, 'plots'))\n",
        "\n",
        "    # Test encoding\n",
        "    test_text = \"आप कैसे हैं?\"\n",
        "    encoded = bpe.encode(test_text)\n",
        "    decoded = bpe.decode(encoded)\n",
        "\n",
        "    print(f\"\\nTest encoding/decoding:\")\n",
        "    print(f\"Original: {test_text}\")\n",
        "    print(f\"Encoded: {encoded}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWtkVJCPnZUN",
        "outputId": "b8a1d144-53ce-4336-8e95-dcfc5afa703b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: models/hindi_bpe\n",
            "Created directory: stats/hindi_bpe\n",
            "Created directory: stats/hindi_bpe/plots\n",
            "Created directory: data/hindi\n",
            "\n",
            "Iteration 0:\n",
            "Vocab size: 78\n",
            "Compression ratio: 1.01\n",
            "New token: ्र (freq: 2,256)\n",
            "Current tokens: 164,917\n",
            "--------------------------------------------------\n",
            "\n",
            "Iteration 500:\n",
            "Vocab size: 578\n",
            "Compression ratio: 2.13\n",
            "New token: त्मक (freq: 35)\n",
            "Current tokens: 78,381\n",
            "--------------------------------------------------\n",
            "\n",
            "Iteration 1,000:\n",
            "Vocab size: 1,078\n",
            "Compression ratio: 2.50\n",
            "New token: सिंह, (freq: 16)\n",
            "Current tokens: 66,835\n",
            "--------------------------------------------------\n",
            "\n",
            "Iteration 1,500:\n",
            "Vocab size: 1,578\n",
            "Compression ratio: 2.76\n",
            "New token: अर्था (freq: 10)\n",
            "Current tokens: 60,560\n",
            "--------------------------------------------------\n",
            "\n",
            "Iteration 2,000:\n",
            "Vocab size: 2,078\n",
            "Compression ratio: 2.96\n",
            "New token: वल्लभ (freq: 7)\n",
            "Current tokens: 56,541\n",
            "--------------------------------------------------\n",
            "\n",
            "Iteration 2,500:\n",
            "Vocab size: 2,578\n",
            "Compression ratio: 3.11\n",
            "New token: अर्थात् (freq: 5)\n",
            "Current tokens: 53,714\n",
            "--------------------------------------------------\n",
            "\n",
            "Iteration 3,000:\n",
            "Vocab size: 3,078\n",
            "Compression ratio: 3.24\n",
            "New token: चलती (freq: 4)\n",
            "Current tokens: 51,522\n",
            "--------------------------------------------------\n",
            "\n",
            "Iteration 3,500:\n",
            "Vocab size: 3,578\n",
            "Compression ratio: 3.35\n",
            "New token: वही (freq: 3)\n",
            "Current tokens: 49,846\n",
            "--------------------------------------------------\n",
            "\n",
            "Iteration 4,000:\n",
            "Vocab size: 4,078\n",
            "Compression ratio: 3.46\n",
            "New token: नारायण, (freq: 3)\n",
            "Current tokens: 48,346\n",
            "--------------------------------------------------\n",
            "\n",
            "Iteration 4,500:\n",
            "Vocab size: 4,578\n",
            "Compression ratio: 3.53\n",
            "New token: जम्मू (freq: 2)\n",
            "Current tokens: 47,321\n",
            "--------------------------------------------------\n",
            "\n",
            "Iteration 5,000:\n",
            "Vocab size: 5,078\n",
            "Compression ratio: 3.61\n",
            "New token: ताक (freq: 2)\n",
            "Current tokens: 46,321\n",
            "--------------------------------------------------\n",
            "\n",
            "Iteration 5,423:\n",
            "Vocab size: 5,500\n",
            "Compression ratio: 3.68\n",
            "New token: जोहान् (freq: 2)\n",
            "Current tokens: 45,477\n",
            "--------------------------------------------------\n",
            "\n",
            "Generated visualization plots in: stats/hindi_bpe/plots\n",
            "\n",
            "Test encoding/decoding:\n",
            "Original: आप कैसे हैं?\n",
            "Encoded: ['आप', 'कैसे', 'हैं', '?']\n",
            "Decoded: आ प क ै स े ह ै ं ?\n"
          ]
        }
      ]
    }
  ]
}